{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import tracemalloc\n",
    "import subprocess\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",torch_dtype=torch.float16).to(device)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deterministic_variants(filter_code):\n",
    "    print(\"Generating deterministic code variants...\")\n",
    "    variants = []\n",
    "    \n",
    "    # Prompt per riduzione degli attributi in Mistral\n",
    "    prompt_attr_reduction = f\"\"\"\n",
    "    [INST]The following definition refers to data minimization: \n",
    "    'Data minimization is a principle restricting data collection to what is necessary \n",
    "    in relation to the purposes for which they are processed.'\n",
    "    Please optimize the provided JavaScript code by reducing unnecessary attributes,\n",
    "    ensuring that only essential data attributes are collected, processed, and stored.\n",
    "    If the code already complies with data minimization,\n",
    "    please add a comment '// No changes needed' to indicate that no modifications are required.\n",
    "    Return only the JavaScript code, without any additional explanations, comments, or introductory text.\n",
    "    Here is the code to optimize:\n",
    "\n",
    "    {filter_code}\n",
    "\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt per riduzione delle funzioni API in Mistral\n",
    "    prompt_api_reduction = f\"\"\"\n",
    "    <s>[INST]The following definition refers to data minimization: \n",
    "    'Data minimization is a principle restricting data collection to what is necessary\n",
    "    in relation to the purposes for which they are processed.'\n",
    "    Please optimize the provided JavaScript code by reducing unnecessary API function calls,\n",
    "    keeping only those needed for minimal and efficient data processing.\n",
    "    If the code already complies with data minimization,\n",
    "    please add a comment '// No changes needed' to indicate that no modifications are required.\n",
    "    Return only the JavaScript code, without any additional explanations, comments, or introductory text.\n",
    "    Here is the code to optimize:\n",
    "\n",
    "    {filter_code}\n",
    "\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt per riduzione del livello di anonimizzazione degli attributi in Mistral\n",
    "    prompt_anon_reduction = f\"\"\"\n",
    "    <s>[INST]The following definition refers to data minimization: \n",
    "    'Data minimization is a principle restricting data collection to what is necessary\n",
    "    in relation to the purposes for which they are processed.'\n",
    "    Please optimize the provided JavaScript code by reducing the level of data anonymization applied to data attributes,\n",
    "    but only if excessive anonymization is present without clear utility.\n",
    "    If the code already complies with data minimization,\n",
    "    please add a comment '// No changes needed' to indicate that no modifications are required.\n",
    "    Return only the JavaScript code, without any additional explanations, comments, or introductory text.\n",
    "    Here is the code to optimize:\n",
    "\n",
    "    {filter_code}\n",
    "\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = [prompt_attr_reduction, prompt_api_reduction, prompt_anon_reduction]\n",
    "    \n",
    "    seed_value = 42\n",
    "    torch.manual_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        #print(f\"Generating variant {i+1} based on prompt {i+1}...\")\n",
    "        \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_length=1000,\n",
    "            temperature=0.0, \n",
    "            top_k=5,         \n",
    "            top_p=0.8\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        cleaned_response = response[len(prompt):].strip()\n",
    "        \n",
    "        if \"// No changes needed\" in cleaned_response:\n",
    "            #print(f\"Variant {i+1} indicates no changes are needed.\")\n",
    "            cleaned_response = f\"{filter_code}\"\n",
    "        #else:\n",
    "            #print(f\"Generated variant {i+1}: {cleaned_response[:100]}...\")\n",
    "        \n",
    "        variants.append(cleaned_response)\n",
    "\n",
    "    return variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_non_deterministic_variants(filter_code):\n",
    "    print(\"Generating non deterministic code variants...\")\n",
    "    variants = []\n",
    "    \n",
    "    # Prompt per riduzione degli attributi in Mistral\n",
    "    prompt_attr_reduction = f\"\"\"\n",
    "    [INST]The following definition refers to data minimization: \n",
    "    'Data minimization is a principle restricting data collection to what is necessary \n",
    "    in relation to the purposes for which they are processed.'\n",
    "    Please optimize the provided JavaScript code by reducing unnecessary attributes,\n",
    "    ensuring that only essential data attributes are collected, processed, and stored.\n",
    "    If the code already complies with data minimization,\n",
    "    please add a comment '// No changes needed' to indicate that no modifications are required.\n",
    "    Return only the JavaScript code, without any additional explanations, comments, or introductory text.\n",
    "    Here is the code to optimize:\n",
    "\n",
    "    {filter_code}\n",
    "\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt per riduzione delle funzioni API in Mistral\n",
    "    prompt_api_reduction = f\"\"\"\n",
    "    <s>[INST]The following definition refers to data minimization: \n",
    "    'Data minimization is a principle restricting data collection to what is necessary\n",
    "    in relation to the purposes for which they are processed.'\n",
    "    Please optimize the provided JavaScript code by reducing unnecessary API function calls,\n",
    "    keeping only those needed for minimal and efficient data processing.\n",
    "    If the code already complies with data minimization,\n",
    "    please add a comment '// No changes needed' to indicate that no modifications are required.\n",
    "    Return only the JavaScript code, without any additional explanations, comments, or introductory text.\n",
    "    Here is the code to optimize:\n",
    "\n",
    "    {filter_code}\n",
    "\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt per riduzione del livello di anonimizzazione degli attributi in Mistral\n",
    "    prompt_anon_reduction = f\"\"\"\n",
    "    <s>[INST]The following definition refers to data minimization: \n",
    "    'Data minimization is a principle restricting data collection to what is necessary\n",
    "    in relation to the purposes for which they are processed.'\n",
    "    Please optimize the provided JavaScript code by reducing the level of data anonymization applied to data attributes,\n",
    "    but only if excessive anonymization is present without clear utility.\n",
    "    If the code already complies with data minimization,\n",
    "    please add a comment '// No changes needed' to indicate that no modifications are required.\n",
    "    Return only the JavaScript code, without any additional explanations, comments, or introductory text.\n",
    "    Here is the code to optimize:\n",
    "\n",
    "    {filter_code}\n",
    "\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "    prompts = [prompt_attr_reduction, prompt_api_reduction, prompt_anon_reduction]\n",
    "    \n",
    "    seed_value = random.randint(0, 10000)\n",
    "    torch.manual_seed(seed_value)\n",
    "    random.seed(seed_value)\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        #print(f\"Generating variant {i+1} based on prompt {i+1}...\")\n",
    "        \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_length=1000,\n",
    "            temperature=0.1,\n",
    "            top_k=5,         \n",
    "            top_p=0.8\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        cleaned_response = response[len(prompt):].strip()\n",
    "        \n",
    "        if \"// No changes needed\" in cleaned_response:\n",
    "            #print(f\"Variant {i+1} indicates no changes are needed.\")\n",
    "            cleaned_response = f\"{filter_code}\"\n",
    "        #else:\n",
    "            #print(f\"Generated variant {i+1}\")\n",
    "        \n",
    "        variants.append(cleaned_response)\n",
    "\n",
    "    return variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attributes(js_code):\n",
    "    attributes = re.findall(r'var\\s+(\\w+)', js_code)\n",
    "    return set(attributes)\n",
    "\n",
    "def count_attributes(js_code):\n",
    "    attributes=extract_attributes(js_code)\n",
    "    attributes_count=len(attributes)\n",
    "    function_count = len(re.findall(r':\\s*function\\b|:\\s*\\([^)]*\\)\\s*=>', js_code))\n",
    "    all_properties = re.findall(r'\\b\\w+\\s*:', js_code)\n",
    "    property_count = len(all_properties) - function_count\n",
    "    return attributes_count+property_count\n",
    "\n",
    "def count_function_calls(js_code):\n",
    "    function_call_pattern = r'\\b\\w+\\.\\w+\\s*\\.\\w+\\s*\\(.*?\\)\\s*;?'\n",
    "    function_calls = re.findall(function_call_pattern, js_code)\n",
    "    return len(function_calls)\n",
    "\n",
    "\n",
    "def analyze_filter_code(js_code):\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        process = subprocess.run([\"node\", \"-e\", js_code], capture_output=True, text=True)\n",
    "        \n",
    "        if process.returncode != 0:\n",
    "            print(f\"JavaScript error: {process.stderr}\")\n",
    "            return None,None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during code execution: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    end_time = time.time()\n",
    "    current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    execution_time = end_time - start_time\n",
    "    peak_memory_kb = peak_memory / 1024\n",
    "    \n",
    "    return execution_time, peak_memory_kb\n",
    "\n",
    "def generate_variants(filter_code, n_variants=3):\n",
    "    variants = []\n",
    "\n",
    "\n",
    "    text = f\"\"\"\n",
    "    [INST]\n",
    "    The following definition refers to data minimization: \n",
    "    'Data minimization is a principle restricting data collection to what is necessary in relation to the purposes for which they are processed.'\n",
    "    Apply data minimization principles to the following JavaScript code according to these criteria:\n",
    "    - Reduce anonymization to retain necessary utility only.\n",
    "    - Keep only essential API calls for minimal, efficient processing.\n",
    "    - Remove unnecessary data attributes, ensuring only essential ones are processed.\n",
    "    If compliant with data minimization, add '// No changes needed.'\n",
    "    Only output the optimized JavaScript code without comments or introduction.\n",
    "\n",
    "    Code:\n",
    "    {filter_code}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    for i in range(n_variants):\n",
    "        print(f\"Generating variant {i+1}...\")\n",
    "        \n",
    "        seed_value = random.randint(0, 10000)\n",
    "        torch.manual_seed(seed_value)\n",
    "        random.seed(seed_value)\n",
    "        \n",
    "\n",
    "        \n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_length=1000, \n",
    "            temperature=0.1,\n",
    "            top_k=5,         \n",
    "            top_p=0.8\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        cleaned_response = response[len(text):].strip()\n",
    "\n",
    "        if \"// No changes needed\" in cleaned_response:\n",
    "            #print(f\"Variant {i+1} indicates no changes are needed.\")\n",
    "            cleaned_response = f\"{filter_code} // No changes needed\"\n",
    "        #else:\n",
    "           #print(f\"Generated variant {i+1}\")\n",
    "\n",
    "        variants.append(cleaned_response)\n",
    "\n",
    "    return variants\n",
    "\n",
    "#Score \n",
    "def evaluate_code(filter_code):\n",
    "    execution_time, memory_used = analyze_filter_code(filter_code)\n",
    "    if execution_time is None:\n",
    "        return None\n",
    "    number_attributes = count_attributes(filter_code)\n",
    "    api_calls = count_function_calls(filter_code)\n",
    "        \n",
    "    score = (\n",
    "        execution_time * 0.1 + \n",
    "        memory_used * 0.1 + \n",
    "        number_attributes * 0.5 + \n",
    "        api_calls * 0.3 \n",
    "    )\n",
    "    \n",
    "    print(f\"Calculated score: {score}\")\n",
    "    return score\n",
    "\n",
    "def save(iteration, mode, codes, name_code):\n",
    "    if mode == 1:\n",
    "        folder_path='Test/Mistral/Test_singolo_prompt/'+str(name_code)+'/Iterazione_'+str(iteration)\n",
    "    elif mode == 2:\n",
    "        folder_path='Test/Mistral/Test_3_prompt_deterministico/'+str(name_code)+'/Iterazione_'+str(iteration)\n",
    "    else:\n",
    "        folder_path='Test/Mistral/Test_3_prompt_non_deterministico/'+str(name_code)+'/Iterazione_'+str(iteration)\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    for i, variant in enumerate(codes):\n",
    "        file_path = os.path.join(folder_path, f\"variant_{i+1}.js\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(variant)\n",
    "\n",
    "def beam_search(initial_code, mode, name_code, beam_width=3, iterations=3):\n",
    "    print(\"Starting beam search...\")\n",
    "    beam = [(initial_code, evaluate_code(initial_code))]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print(f\"\\n--- Iteration {i+1} ---\")\n",
    "        candidates = []\n",
    "        \n",
    "        for code, score in beam:\n",
    "            if mode==1:\n",
    "                print(f\"Generating variants with generate_variants...\")\n",
    "                variants = generate_variants(code)\n",
    "            elif mode==2:\n",
    "                print(f\"Generating variants with generate_deterministic_variants...\")\n",
    "                variants = generate_deterministic_variants(code)\n",
    "            else:\n",
    "                print(f\"Generating variants with generate_non_deterministic_variants...\")\n",
    "                variants = generate_non_deterministic_variants(code)\n",
    "\n",
    "            \n",
    "\n",
    "            for variant in variants:\n",
    "                variant_score = evaluate_code(variant)\n",
    "                if variant_score is None:\n",
    "                    candidates.append((code, evaluate_code(code)))\n",
    "                else:\n",
    "                    candidates.append((variant, variant_score))\n",
    "        \n",
    "        candidates.sort(key=lambda x: x[1] if x[1] is not None else float('+inf'))\n",
    "        beam = candidates[:min(beam_width, len(candidates))]\n",
    "        codes = [code for code,score in beam]\n",
    "        save(i+1,mode, codes, name_code)\n",
    "\n",
    "            \n",
    "    print(\"\\nBeam search completed.\")\n",
    "    return beam[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_rates(original_code, LLM_code, manual_code):\n",
    "    original_attributes = extract_attributes(original_code)\n",
    "    LLM_attributes = extract_attributes(LLM_code)\n",
    "    manual_attributes = extract_attributes(manual_code)\n",
    "    \n",
    "    attributes_to_remove = original_attributes - manual_attributes\n",
    "    total_possible_removals = len(attributes_to_remove)\n",
    "    print(f'Il numero di attributi che possono essere rimossi è {total_possible_removals}\\n')\n",
    "    attributes_not_removed_by_LLM = [attr for attr in attributes_to_remove if attr in LLM_attributes]\n",
    "    print(f'Il numero di attributi che il LLM non ha rimosso è {len(attributes_not_removed_by_LLM)}\\n')\n",
    "\n",
    "    #Primo error rate\n",
    "    error_rate_1 = len(attributes_not_removed_by_LLM) / total_possible_removals if total_possible_removals > 0 else 0\n",
    "    necessary_attributes = manual_attributes.intersection(original_attributes)\n",
    "    print(f'Il numero di attributi necessari è {len(necessary_attributes)}\\n')\n",
    "    necessary_not_removed_by_LLM = [attr for attr in necessary_attributes if attr in LLM_attributes]\n",
    "    print(f'Il numero di attributi necessari non rimossi dal LLM è {len(necessary_not_removed_by_LLM)}\\n')\n",
    "\n",
    "    #Secondo error rate\n",
    "    total_necessary = len(necessary_attributes)\n",
    "    error_rate_2 = 1-len(necessary_not_removed_by_LLM) / total_necessary if total_necessary > 0 else 0\n",
    "\n",
    "    return error_rate_1, error_rate_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_filter_codes_50 = 'filter_codes_50'\n",
    "folder_filter_code_test='filter_code_test'\n",
    "files = [f for f in os.listdir(folder_filter_codes_50) if os.path.isfile(os.path.join(folder_filter_codes_50, f))]\n",
    "for file in files:\n",
    "    with open(folder_filter_codes_50+'/'+file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            print(file)\n",
    "            code, score = beam_search(content,2,file) #Cambiare mode per ogni test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def evaluate_all_filters(base_dir):\n",
    "    best_variants = {}\n",
    "    \n",
    "    for filter_name in os.listdir(base_dir):\n",
    "        filter_path = os.path.join(base_dir, filter_name)\n",
    "        \n",
    "        if os.path.isdir(filter_path):\n",
    "            iter3_path = os.path.join(filter_path, 'Iterazione_3')\n",
    "            \n",
    "            if os.path.isdir(iter3_path):\n",
    "                best_score = float('inf')\n",
    "                best_variant = None\n",
    "                \n",
    "                for variant in ['variant_1.js', 'variant_2.js', 'variant_3.js']:\n",
    "                    variant_path = os.path.join(iter3_path, variant)\n",
    "                    \n",
    "                    with open(variant_path, 'r', encoding='utf-8') as file:\n",
    "                        code = file.read()\n",
    "                    \n",
    "                    score = evaluate_code(code)\n",
    "                    \n",
    "                    if score is not None and score < best_score:\n",
    "                        best_score = score\n",
    "                        best_variant = variant_path\n",
    "                \n",
    "                best_variants[filter_name] = (best_variant, best_score)\n",
    "    \n",
    "    return best_variants\n",
    "\n",
    "def errors(original_code_path, best_code, test_code_path):\n",
    "    with open(original_code_path, 'r',  encoding='utf-8') as original:\n",
    "        original_code=original.read()\n",
    "        with open(test_code_path,'r', encoding='utf-8') as test:\n",
    "            test_code=test.read()\n",
    "            with open(best_code,'r', encoding='utf-8') as LLM:\n",
    "                LLM_code=LLM.read()\n",
    "                error_rate_var_removed, error_rate_var_removed_necessary = calculate_error_rates(original_code,LLM_code,test_code)\n",
    "    return error_rate_var_removed, error_rate_var_removed_necessary\n",
    "\n",
    "\n",
    "base_directory = 'Test/Mistral/Test_singolo_prompt/' #Cambiare per ogni test\n",
    "text_file_path='Risultati/Mistral/error_rate_generate_variants.txt' #Cambiare per ogni test\n",
    "best_variants = evaluate_all_filters(base_directory)\n",
    "with open(text_file_path,'a',encoding='utf-8') as error_file:\n",
    "    error_file.write('filter_name,error_rate_1,error_rate_2\\n')\n",
    "    for filter_name, (best_variant, score) in best_variants.items():\n",
    "        print(f\"Best variant for {filter_name}: {best_variant} with score {score}\")\n",
    "        original_code_path='filter_codes_50/' + filter_name\n",
    "        test_file_name = filter_name.replace('.js', '_test.js')\n",
    "        test_code_path='filter_code_test/' + test_file_name\n",
    "        error_rate_1, error_rate_2=errors(original_code_path,best_variant,test_code_path)\n",
    "        error_file.write(f\"{filter_name},{error_rate_1},{error_rate_2}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
